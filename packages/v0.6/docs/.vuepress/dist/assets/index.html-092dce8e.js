import{_ as p,E as o,Z as c,$ as r,a0 as e,a3 as a,a1 as s,a2 as i,a4 as l}from"./framework-64cb0dab.js";const d="/assets/quick-tutorial-1-3eecb59a.png",u="/assets/quick-tutorial-2-57effcf2.png",h="/assets/quick-tutorial-3-9d6cfe63.png",k="/assets/quick-tutorial-4-d0a10ebe.png",m="/assets/quick-tutorial-5-bc045d29.png",g="/assets/quick-tutorial-6-46741e6a.png",b="/assets/quick-tutorial-7-6bc2c386.png",v="/assets/quick-tutorial-8-7d7a1ebe.png",w="/assets/quick-tutorial-9-aa3228b7.png",y="/assets/quick-tutorial-10-abdc0eaf.png",_={},f=e("h1",{id:"quick-tutorial",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#quick-tutorial","aria-hidden":"true"},"#"),a(" Quick Tutorial")],-1),q=e("p",null,"You have now installed Crawlab and perhaps can't wait to start using it. Before you go deep into the details, I would suggest you go through this quick tutorial which would walk you through some basics and get you familiar with some main features in Crawlab.",-1),x=e("h2",{id:"introduction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#introduction","aria-hidden":"true"},"#"),a(" Introduction")],-1),C={href:"http://quotes.toscrape.com/",target:"_blank",rel:"noopener noreferrer"},S={href:"https://www.zyte.com/",target:"_blank",rel:"noopener noreferrer"},N={href:"https://scrapy.org/",target:"_blank",rel:"noopener noreferrer"},T={class:"hint-container warning"},I=e("p",{class:"hint-container-title"},"Note",-1),L={href:"https://www.python.org/",target:"_blank",rel:"noopener noreferrer"},V={href:"https://pip.pypa.io/en/stable/installation/",target:"_blank",rel:"noopener noreferrer"},j=l(`<h2 id="create-spider" tabindex="-1"><a class="header-anchor" href="#create-spider" aria-hidden="true">#</a> Create Spider</h2><p>First thing first, we are going to generate a Scrapy project. Let&#39;s start by installing Scrapy.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> scrapy
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Then, let&#39;s create a Scrapy project called <code>scrapy_quotes</code>.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>scrapy startproject scrapy_quotes
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Now you should be able to see the folder structure of the newly created spider.</p><p>Then generate a new spider by executing the command below.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> scrapy_quotes
scrapy genspider quotes quotes.toscrape.com
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>The folder structure of the spider should look similar as below.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>scrapy_quotes
├── scrapy.cfg
└── scrapy_quotes
    ├── __init__.py
    ├── items.py
    ├── middlewares.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
        ├── __init__.py
        └── quotes.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Let&#39;s open file <code>scrapy_quotes/scrapy_quotes/spiders/quotes.py</code> and replace original content with the code below.</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># scrapy_quotes/scrapy_quotes/spiders/quotes.py</span>
<span class="token keyword">import</span> scrapy

<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">&#39;quotes&#39;</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;quotes.toscrape.com&#39;</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;http://quotes.toscrape.com/&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">&quot;div.quote&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">yield</span> <span class="token punctuation">{</span>
                <span class="token string">&#39;text&#39;</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">&quot;span.text::text&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">&#39;author&#39;</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">&quot;small.author::text&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">&#39;tags&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;,&#39;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">&quot;div.tags &gt; a.tag::text&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token punctuation">}</span>

        next_page_url <span class="token operator">=</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">&quot;li.next &gt; a::attr(href)&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> next_page_url <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>response<span class="token punctuation">.</span>urljoin<span class="token punctuation">(</span>next_page_url<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Then open file <code>scrapy_quotes/scrapy_quotes/settings.py</code> and add the code below at the end of the file.</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># add at the end of scrapy_quotes/scrapy_quotes/settings.py</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&#39;crawlab.CrawlabPipeline&#39;</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>We have now created a Scrapy spider that is available for integration into Crawlab. We can then upload it to Crawlab.</p><h2 id="upload-spider" tabindex="-1"><a class="header-anchor" href="#upload-spider" aria-hidden="true">#</a> Upload Spider</h2><p>Now open the browser and navigate to <code>http://localhost:8080</code>, and login with default username/password <code>admin/admin</code>.</p><p>Then go to <code>Spiders</code> page, and click <code>New Spider</code> on the top left to open the create dialog.</p><p>In the dialog, enter the info as below, and click <code>Confirm</code> to create a spider on Crawlab.</p><ol><li>Name: <code>scrapy_quotes</code></li><li>Command: <code>scrapy crawl quotes</code></li></ol><p><img src="`+d+'" alt="create-spider"></p><p>Now we have created a spider in Crawlab, and we need to upload spider files to it.</p><p>Click the spider name link or the blue button with search icon on the right named <code>View</code> to navigate to its detail page.</p><p><img src="'+u+'" alt=""></p><p>In the detail page, click <code>Files</code> tab to navigate to the file editor tab, and click <code>Upload Files</code> button on the nav bar.</p><p><img src="'+h+'" alt=""></p><p>Click the button <code>Click to Select Folder to Upload</code> and choose the folder of the Scrapy spider we created earlier, and click <code>Confirm</code> to upload the spiders.</p><p><img src="'+k+'" alt=""></p><p>Now you should be able to see the uploaded files and folders on the left file navigator. You can play around it by expanding folder or double-clicking files to open it.</p><p><img src="'+m+'" alt=""></p><h2 id="run-spider" tabindex="-1"><a class="header-anchor" href="#run-spider" aria-hidden="true">#</a> Run Spider</h2><p>Running spider is quite simple.</p><p>Let&#39;s run it by clicking the button with play icon <code>Run</code>, and click <code>Confirm</code> with default settings.</p><p><img src="'+g+'" alt=""></p><h2 id="view-task" tabindex="-1"><a class="header-anchor" href="#view-task" aria-hidden="true">#</a> View Task</h2><p>Great! We have just triggered a spider task, and we should be able to view it in <code>Tasks</code> tab. Let&#39;s click on <code>Tasks</code> tab to take a look.</p><p><img src="'+b+'" alt=""></p><p>The task is running, and we can view the task detail by clicking the <code>View</code> button on the right.</p><p>We can then view realtime logs by clicking <code>Logs</code> tab.</p><p><img src="'+v+'" alt=""></p><p>As the spider is crawling, we can also view crawled data by clicking <code>Data</code> tab;</p><p><img src="'+w+'" alt=""></p><h2 id="spider-stats" tabindex="-1"><a class="header-anchor" href="#spider-stats" aria-hidden="true">#</a> Spider Stats</h2><p>If we go back to <code>Spiders</code> page, we should be able to see the stats of the spider.</p><p><img src="'+y+'" alt=""></p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>Let&#39;s do some recap here. We have followed a simple process to run a Scrapy spider on Crawlab.</p><ol><li>Created a Scrapy project on local.</li><li>Created a spider on Crawlab</li><li>Uploaded spider files to the spider</li><li>Triggered a task</li><li>View realtime logs and crawled data along with spider stats on Crawlab.</li></ol><h2 id="what-next" tabindex="-1"><a class="header-anchor" href="#what-next" aria-hidden="true">#</a> What Next</h2><p>This quick tutorial provides you with a basic understanding about Crawlab and how to use it to crawl data, including monitoring task logs and view crawled data, etc. But you should be aware that Crawlab is not only limited to that, there are many other powerful features such as Node Management, Git Integration and Dependency Installation which you might be interested in. You can check out other sections of this guide to learn how to use those features.</p>',50);function E(W,R){const n=o("ExternalLinkIcon"),t=o("RouterLink");return c(),r("div",null,[f,q,x,e("p",null,[a("In this tutorial, we are going to create a spider that crawls quotes on a "),e("a",C,[a("mock site"),s(n)]),a(" provided by "),e("a",S,[a("Zyte"),s(n)]),a(" (the company behind Scrapy); then we will upload this spider to Crawlab, and run it to extract quotes data; finally, we will view the crawled data visually on Crawlab.")]),e("p",null,[a("The framework we are going to use is "),e("a",N,[a("Scrapy"),s(n)]),a(", the most popular web crawler framework written in Python, which is easy to use yet with many powerful features.")]),e("div",T,[I,e("p",null,[a("We assume you have installed Crawlab on your local machine by following "),s(t,{to:"/en/guide/quick-start.html"},{default:i(()=>[a("Quick Start")]),_:1}),a(". If you haven't, please refer to "),s(t,{to:"/en/guide/quick-start.html"},{default:i(()=>[a("Quick Start")]),_:1}),a(" to install it on your local machine.")]),e("p",null,[a("As we are using Scrapy, please make sure you have installed "),e("a",L,[a("Python"),s(n)]),a(" (>=3.6) and module management tool "),e("a",V,[a("pip"),s(n)]),a(" before proceeding any further steps.")])]),j])}const P=p(_,[["render",E],["__file","index.html.vue"]]);export{P as default};
